{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:38:21.707234Z",
     "start_time": "2020-03-31T15:38:18.947959Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import os, pickle\n",
    "\n",
    "# specify the GPU device\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:38:21.715617Z",
     "start_time": "2020-03-31T15:38:21.709757Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Config\n",
    "'''\n",
    "# batch size per iteration\n",
    "BATCHSIZE = 200\n",
    "# mini-batch size for few-shot learning\n",
    "MINIBATCHSIZE = 20 \n",
    "# learning rate\n",
    "LR = 1e-3 \n",
    "# coefficient to balance `cold-start' and `warm-up'\n",
    "ALPHA = 0.1\n",
    "# length of embedding vectors\n",
    "EMB_SIZE = 128\n",
    "# model\n",
    "MODEL = 'deepFM'\n",
    "# log file\n",
    "LOG = \"logs/{}.csv\".format(MODEL)\n",
    "# path to save the model\n",
    "saver_path =\"saver/model-\"+LOG.split(\"/\")[-1][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:38:21.725157Z",
     "start_time": "2020-03-31T15:38:21.718241Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_pkl(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        t = pickle.load(f)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:38:29.003436Z",
     "start_time": "2020-03-31T15:38:21.727628Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# training data of big ads\n",
    "train = read_pkl(\"../data/big_train_main.pkl\")\n",
    "# some pre-processing\n",
    "num_words_dict = {\n",
    "    'MovieID': 4000,\n",
    "    'UserID': 6050,\n",
    "    'Age': 7,\n",
    "    'Gender': 2,\n",
    "    'Occupation': 21,\n",
    "    'Year': 83,\n",
    "}\n",
    "ID_col = 'MovieID'\n",
    "item_col = ['Year']\n",
    "context_col = ['Age', 'Gender', 'Occupation', 'UserID']\n",
    "train_y = train['y']\n",
    "train_x = train[[ID_col]+item_col+context_col]\n",
    "train_t = pad_sequences(train.Title, maxlen=8)\n",
    "train_g = pad_sequences(train.Genres, maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>y</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[53, 2415, 312, 1, 2416, 2417]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[957, 5, 1, 332, 1969]</td>\n",
       "      <td>[16, 10, 14]</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[13, 538, 217]</td>\n",
       "      <td>[14, 5]</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[4142, 4143]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[3248, 18, 3]</td>\n",
       "      <td>[16, 10, 2]</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  y  Gender  Age  Occupation  \\\n",
       "0       1     1193  1       0    0          10   \n",
       "1       1      661  0       0    0          10   \n",
       "2       1      914  0       0    0          10   \n",
       "3       1     3408  1       0    0          10   \n",
       "4       1     2355  1       0    0          10   \n",
       "\n",
       "                            Title        Genres  Year  \n",
       "0  [53, 2415, 312, 1, 2416, 2417]           [1]    57  \n",
       "1          [957, 5, 1, 332, 1969]  [16, 10, 14]    78  \n",
       "2                  [13, 538, 217]       [14, 5]    46  \n",
       "3                    [4142, 4143]           [1]    82  \n",
       "4                   [3248, 18, 3]   [16, 10, 2]    80  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:38:30.925695Z",
     "start_time": "2020-03-31T15:38:29.005891Z"
    }
   },
   "outputs": [],
   "source": [
    "# few-shot data for the small ads\n",
    "test_a = read_pkl(\"../data/test_oneshot_a.pkl\")\n",
    "test_b = read_pkl(\"../data/test_oneshot_b.pkl\")\n",
    "test_c = read_pkl(\"../data/test_oneshot_c.pkl\")\n",
    "test_test = read_pkl(\"../data/test_test.pkl\")\n",
    "\n",
    "test_x_a = test_a[[ID_col]+item_col+context_col]\n",
    "test_y_a = test_a['y'].values\n",
    "test_t_a = pad_sequences(test_a.Title, maxlen=8)\n",
    "test_g_a = pad_sequences(test_a.Genres, maxlen=4)\n",
    "\n",
    "test_x_b = test_b[[ID_col]+item_col+context_col]\n",
    "test_y_b = test_b['y'].values\n",
    "test_t_b = pad_sequences(test_b.Title, maxlen=8)\n",
    "test_g_b = pad_sequences(test_b.Genres, maxlen=4)\n",
    "\n",
    "test_x_c = test_c[[ID_col]+item_col+context_col]\n",
    "test_y_c = test_c['y'].values\n",
    "test_t_c = pad_sequences(test_c.Title, maxlen=8)\n",
    "test_g_c = pad_sequences(test_c.Genres, maxlen=4)\n",
    "\n",
    "test_x_test = test_test[[ID_col]+item_col+context_col]\n",
    "test_y_test = test_test['y'].values\n",
    "test_t_test = pad_sequences(test_test.Title, maxlen=8)\n",
    "test_g_test = pad_sequences(test_test.Genres, maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:38:30.972485Z",
     "start_time": "2020-03-31T15:38:30.927469Z"
    },
    "code_folding": [
     82
    ]
   },
   "outputs": [],
   "source": [
    "class Meta_Model(object):\n",
    "    def __init__(self, ID_col, item_col, context_col, nb_words, model='FM',\n",
    "                 emb_size=128, alpha=0.1,\n",
    "                 warm_lr=1e-3, cold_lr=1e-4, ME_lr=1e-3):\n",
    "        \"\"\"\n",
    "        ID_col: string, the column name of the item ID\n",
    "        item_col: list, the columns of item features\n",
    "        context_col: list, the columns of other features\n",
    "        nb_words: dict, nb of words in each of these columns\n",
    "        \"\"\"\n",
    "        columns = [ID_col] + item_col + context_col\n",
    "        def get_embeddings():\n",
    "            inputs, tables = {}, []\n",
    "            item_embs, other_embs = [], []\n",
    "            for col in columns:\n",
    "                inputs[col] = tf.placeholder(tf.int32, [None])\n",
    "                table = tf.get_variable(\n",
    "                    \"table_{}\".format(col), [nb_words[col], emb_size],\n",
    "                    initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "                emb = tf.nn.embedding_lookup(table, inputs[col])\n",
    "                if col==ID_col:\n",
    "                    ID_emb = emb\n",
    "                    ID_table = table\n",
    "                elif col in item_col:\n",
    "                    item_embs.append(emb)\n",
    "                else:\n",
    "                    other_embs.append(emb)\n",
    "\n",
    "            inputs[\"title\"] = tf.placeholder(tf.int32, [None, 8])\n",
    "            inputs[\"genres\"] = tf.placeholder(tf.int32, [None, 4])\n",
    "\n",
    "            title_emb = tf.contrib.layers.embed_sequence(\n",
    "                inputs[\"title\"], 20001, emb_size, scope=\"word_emb\")\n",
    "            genre_emb = tf.contrib.layers.embed_sequence(\n",
    "                inputs[\"genres\"], 21, emb_size, scope=\"genre_table\")\n",
    "            item_embs.append(tf.reduce_mean(title_emb, axis=1))\n",
    "            item_embs.append(tf.reduce_mean(genre_emb, axis=1))\n",
    "            \n",
    "            return inputs, ID_emb, item_embs, other_embs, ID_table\n",
    "        \n",
    "        def generate_meta_emb(item_embs):\n",
    "            \"\"\"\n",
    "            This is the simplest architecture of the embedding generator,\n",
    "            with only a dense layer.\n",
    "            You can customize it if you want have a stronger performance, \n",
    "            for example, you can add an l2 regularization term or alter \n",
    "            the pooling layer. \n",
    "            \"\"\"\n",
    "            embs = tf.stop_gradient(tf.stack(item_embs, 1))\n",
    "            item_h = tf.layers.flatten(embs)\n",
    "            emb_pred_Dense = tf.layers.Dense(\n",
    "                emb_size, activation=tf.nn.tanh, use_bias=False,\n",
    "                name='emb_predictor') \n",
    "            emb_pred = emb_pred_Dense(item_h) / 5.\n",
    "            ME_vars = emb_pred_Dense.trainable_variables\n",
    "            return emb_pred, ME_vars\n",
    "\n",
    "        def get_yhat_deepFM(ID_emb, item_embs, other_embs, **kwargs):\n",
    "            embeddings = [ID_emb] + item_embs + other_embs\n",
    "            sum_of_emb = tf.add_n(embeddings)\n",
    "            diff_of_emb = [sum_of_emb - x for x in embeddings]\n",
    "            dot_of_emb = [tf.reduce_sum(embeddings[i]*diff_of_emb[i], \n",
    "                                        axis=1, keepdims=True) \n",
    "                          for i in range(len(columns))]\n",
    "            h = tf.concat(dot_of_emb, 1)\n",
    "            h2 = tf.concat(embeddings, 1)\n",
    "            for i in range(2):\n",
    "                h2 = tf.nn.relu(tf.layers.dense(h2, emb_size, name='deep-{}'.format(i)))\n",
    "            h = tf.concat([h,h2], 1)\n",
    "            y = tf.nn.sigmoid(tf.layers.dense(h, 1, name='out'))\n",
    "            return y\n",
    "        def get_yhat_PNN(ID_emb, item_embs, other_embs, **kwargs):\n",
    "            embeddings = [ID_emb] + item_embs + other_embs\n",
    "            sum_of_emb = tf.add_n(embeddings)\n",
    "            diff_of_emb = [sum_of_emb - x for x in embeddings]\n",
    "            dot_of_emb = [tf.reduce_sum(embeddings[i]*diff_of_emb[i], \n",
    "                                        axis=1, keepdims=True)\n",
    "                          for i in range(len(columns))]\n",
    "            dots = tf.concat(dot_of_emb, 1)\n",
    "            h2 = tf.concat(embeddings, 1)\n",
    "            h = tf.concat([dots,h2], 1)\n",
    "            w = tf.get_variable('MLP_1/kernel', shape=(h.shape[1],emb_size))\n",
    "            b = tf.get_variable('MLP_1/bias', shape=(emb_size,), \n",
    "                                initializer=tf.initializers.zeros)\n",
    "            h = tf.nn.relu(tf.matmul(h,w)+b)\n",
    "            w = tf.get_variable('MLP_2/kernel', shape=(h.shape[1],1))\n",
    "            b = tf.get_variable('MLP_2/bias', shape=(1,), \n",
    "                                initializer=tf.initializers.constant(0.))\n",
    "            y = tf.nn.sigmoid(tf.matmul(h,w)+b)\n",
    "            return y\n",
    "        '''\n",
    "        *CHOOSE THE BASE MODEL HERE*\n",
    "        '''\n",
    "        get_yhat = {\n",
    "            \"PNN\": get_yhat_PNN, \n",
    "            \"deepFM\": get_yhat_deepFM\n",
    "        }[model]\n",
    "        \n",
    "        with tf.variable_scope(\"model\"):\n",
    "            # build the base model\n",
    "            inputs, ID_emb, item_embs, other_embs, ID_table = get_embeddings()\n",
    "            label = tf.placeholder(tf.float32, [None, 1])\n",
    "            # outputs and losses of the base model\n",
    "            yhat = get_yhat(ID_emb, item_embs, other_embs)\n",
    "            warm_loss = tf.losses.log_loss(label, yhat)\n",
    "            # Meta-Embedding: build the embedding generator\n",
    "            meta_ID_emb, ME_vars = generate_meta_emb(item_embs)\n",
    "\n",
    "        with tf.variable_scope(\"model\", reuse=True):\n",
    "            # Meta-Embedding: step 1, cold-start, \n",
    "            #     use the generated meta-embedding to make predictions\n",
    "            #     and calculate the cold-start loss_a\n",
    "            cold_yhat_a = get_yhat(meta_ID_emb, item_embs, other_embs)\n",
    "            cold_loss_a = tf.losses.log_loss(label, cold_yhat_a)\n",
    "            # Meta-Embedding: step 2, apply gradient descent once\n",
    "            #     get the adapted embedding\n",
    "            cold_emb_grads = tf.gradients(cold_loss_a, meta_ID_emb)[0]\n",
    "            meta_ID_emb_new = meta_ID_emb - cold_lr * cold_emb_grads\n",
    "            # Meta-Embedding: step 3, \n",
    "            #     use the adapted embedding to make prediction on another mini-batch \n",
    "            #     and calculate the warm-up loss_b\n",
    "            inputs_b, _, item_embs_b, other_embs_b, _ = get_embeddings()\n",
    "            label_b = tf.placeholder(tf.float32, [None, 1])\n",
    "            cold_yhat_b = get_yhat(meta_ID_emb_new, item_embs_b, other_embs_b)\n",
    "            cold_loss_b = tf.losses.log_loss(label_b, cold_yhat_b)            \n",
    "        \n",
    "        # build the optimizer and update op for the original model\n",
    "        warm_optimizer = tf.train.AdamOptimizer(warm_lr)\n",
    "        warm_update_op = warm_optimizer.minimize(warm_loss)\n",
    "        warm_update_emb_op = warm_optimizer.minimize(warm_loss, var_list=[ID_table])\n",
    "        # build the optimizer and update op for meta-embedding\n",
    "        # Meta-Embedding: step 4, calculate the final meta-loss\n",
    "        ME_loss = cold_loss_a * alpha + cold_loss_b * (1-alpha)\n",
    "        ME_optimizer = tf.train.AdamOptimizer(ME_lr)\n",
    "        ME_update_op = ME_optimizer.minimize(ME_loss, var_list=ME_vars)\n",
    "        \n",
    "        ID_table_new = tf.placeholder(tf.float32, ID_table.shape)\n",
    "        ME_assign_op = tf.assign(ID_table, ID_table_new)\n",
    "        \n",
    "        def predict_warm(sess, X, Title, Genres):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"genres\"]: Genres,\n",
    "                         **feed_dict}\n",
    "            return sess.run(yhat, feed_dict)\n",
    "        def predict_ME(sess, X, Title, Genres):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"genres\"]: Genres,\n",
    "                         **feed_dict}\n",
    "            return sess.run(cold_yhat_a, feed_dict)\n",
    "        def get_meta_embedding(sess, X, Title, Genres):\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"genres\"]: Genres,\n",
    "                         **feed_dict}\n",
    "            return sess.run(meta_ID_emb, feed_dict)\n",
    "        def assign_meta_embedding(sess, ID, emb):\n",
    "            # take the embedding matrix\n",
    "            table = sess.run(ID_table)\n",
    "            # replace the ID^th row by the new embedding\n",
    "            table[ID, :] = emb\n",
    "            return sess.run(ME_assign_op, feed_dict={ID_table_new: table})\n",
    "        def train_warm(sess, X, Title, Genres, y, embedding_only=False):\n",
    "            # original training on batch\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"genres\"]: Genres,\n",
    "                         **feed_dict}\n",
    "            feed_dict[label] = y.reshape((-1,1))\n",
    "            return sess.run([\n",
    "                warm_loss, warm_update_emb_op if embedding_only else warm_update_op \n",
    "            ], feed_dict=feed_dict)\n",
    "        def train_ME(sess, X, Title, Genres, y, \n",
    "                     X_b, Title_b, Genres_b, y_b):\n",
    "            # train the embedding generator\n",
    "            feed_dict = {inputs[col]: X[col] for col in columns}\n",
    "            feed_dict = {inputs[\"title\"]: Title,\n",
    "                         inputs[\"genres\"]: Genres,\n",
    "                         **feed_dict}\n",
    "            feed_dict[label] = y.reshape((-1,1))\n",
    "            feed_dict_b = {inputs_b[col]: X_b[col] for col in columns}\n",
    "            feed_dict_b = {inputs_b[\"title\"]: Title_b,\n",
    "                           inputs_b[\"genres\"]: Genres_b,\n",
    "                           **feed_dict_b}\n",
    "            feed_dict_b[label_b] = y_b.reshape((-1,1))\n",
    "            return sess.run([\n",
    "                cold_loss_a, cold_loss_b, ME_update_op\n",
    "            ], feed_dict={**feed_dict, **feed_dict_b})\n",
    "        self.predict_warm = predict_warm\n",
    "        self.predict_ME = predict_ME\n",
    "        self.train_warm = train_warm\n",
    "        self.train_ME = train_ME\n",
    "        self.get_meta_embedding = get_meta_embedding\n",
    "        self.assign_meta_embedding = assign_meta_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:38:36.053130Z",
     "start_time": "2020-03-31T15:38:30.974006Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-15-9cc79472006e>:68: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/baek/anaconda3/envs/tf-v1/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/baek/anaconda3/envs/tf-v1/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From <ipython-input-15-9cc79472006e>:50: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    }
   ],
   "source": [
    "model = Meta_Model(ID_col, item_col, context_col, num_words_dict, model=MODEL,\n",
    "                   emb_size=EMB_SIZE, alpha=ALPHA,\n",
    "                   warm_lr=LR, cold_lr=LR/10., ME_lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:38:36.065886Z",
     "start_time": "2020-03-31T15:38:36.055626Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def predict_on_batch(sess, predict_func, test_x, test_t, test_g, batchsize=800):\n",
    "    n_samples_test = test_x.shape[0]\n",
    "    n_batch_test = n_samples_test//batchsize\n",
    "    test_pred = np.zeros(n_samples_test)\n",
    "    for i_batch in range(n_batch_test):\n",
    "        batch_x = test_x.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t = test_t[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_g = test_g[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        _pred = predict_func(sess, batch_x, batch_t, batch_g)\n",
    "        test_pred[i_batch*batchsize:(i_batch+1)*batchsize] = _pred.reshape(-1)\n",
    "    if n_batch_test*batchsize<n_samples_test:\n",
    "        batch_x = test_x.iloc[n_batch_test*batchsize:]\n",
    "        batch_t = test_t[n_batch_test*batchsize:]\n",
    "        batch_g = test_g[n_batch_test*batchsize:]\n",
    "        _pred = predict_func(sess, batch_x, batch_t, batch_g)\n",
    "        test_pred[n_batch_test*batchsize:] = _pred.reshape(-1)\n",
    "    return test_pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:39:09.317983Z",
     "start_time": "2020-03-31T15:38:36.068663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3828/3828 [00:33<00:00, 114.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre-train]\n",
      "\ttest-test loss: 1.185682\n",
      "[pre-train]\n",
      "\ttest-test auc: 0.650687\n",
      "Model saved in path: saver/model-deepFM\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pre-train the base model\n",
    "\"\"\"\n",
    "batchsize = BATCHSIZE\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_samples = train_x.shape[0]\n",
    "n_batch = n_samples//batchsize\n",
    "\n",
    "for i_batch in tqdm(range(n_batch)):\n",
    "    batch_x = train_x.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "    batch_t = train_t[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "    batch_g = train_g[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "    batch_y = train_y.iloc[i_batch*batchsize:(i_batch+1)*batchsize].values\n",
    "    loss, _ = model.train_warm(sess, batch_x, batch_t, batch_g, batch_y)\n",
    "\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_base_cold = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[pre-train]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "auc_base_cold = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[pre-train]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "save_path = saver.save(sess, saver_path)\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:39:09.324012Z",
     "start_time": "2020-03-31T15:39:09.320240Z"
    }
   },
   "outputs": [],
   "source": [
    "minibatchsize = MINIBATCHSIZE\n",
    "batch_n_ID = 25\n",
    "batchsize = minibatchsize*batch_n_ID\n",
    "n_epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:39:14.876141Z",
     "start_time": "2020-03-31T15:39:09.325942Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 42.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 1.163275\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.659381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 66.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 1.123330\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.665855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 69.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss: 1.094998\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc: 0.668996\n",
      "Model saved in path: saver/model-deepFM\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train the Meta-Embedding generator\n",
    "'''\n",
    "best_auc = 0\n",
    "best_loss = 10\n",
    "for i_epoch in range(n_epoch):\n",
    "    # Read the few-shot training data of big ads\n",
    "    if i_epoch==0:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_a.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_b.pkl\")\n",
    "    elif i_epoch==1:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_c.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_d.pkl\")\n",
    "    elif i_epoch==2:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_b.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_c.pkl\")\n",
    "    elif i_epoch==3:\n",
    "        _train_a = read_pkl(\"../data/train_oneshot_d.pkl\")\n",
    "        _train_b = read_pkl(\"../data/train_oneshot_a.pkl\")\n",
    "    train_x_a = _train_a[[ID_col]+item_col+context_col]\n",
    "    train_y_a = _train_a['y'].values\n",
    "    train_t_a = pad_sequences(_train_a.Title, maxlen=8)\n",
    "    train_g_a = pad_sequences(_train_a.Genres, maxlen=4)\n",
    "\n",
    "    train_x_b = _train_b[[ID_col]+item_col+context_col]\n",
    "    train_y_b = _train_b['y'].values\n",
    "    train_t_b = pad_sequences(_train_b.Title, maxlen=8)\n",
    "    train_g_b = pad_sequences(_train_b.Genres, maxlen=4)\n",
    "    \n",
    "    n_samples = train_x_a.shape[0]\n",
    "    n_batch = n_samples//batchsize\n",
    "    # Start training\n",
    "    for i_batch in tqdm(range(n_batch)):\n",
    "        batch_x_a = train_x_a.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t_a = train_t_a[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_g_a = train_g_a[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_y_a = train_y_a[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_x_b = train_x_b.iloc[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_t_b = train_t_b[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_g_b = train_g_b[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        batch_y_b = train_y_b[i_batch*batchsize:(i_batch+1)*batchsize]\n",
    "        loss_a, loss_b, _ = model.train_ME(sess, \n",
    "                                           batch_x_a, batch_t_a, batch_g_a, batch_y_a, \n",
    "                                           batch_x_b, batch_t_b, batch_g_b, batch_y_b, )\n",
    "    # on epoch end\n",
    "    test_pred_test = predict_on_batch(sess, model.predict_ME, \n",
    "                                      test_x_test, test_t_test, test_g_test)\n",
    "    logloss_ME_cold = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "    print(\"[Meta-Embedding]\\n\\ttest-test loss: {:.6f}\".format(test_loss_test))\n",
    "    auc_ME_cold = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "    print(\"[Meta-Embedding]\\n\\ttest-test auc: {:.6f}\".format(test_auc_test))\n",
    "\n",
    "save_path = saver.save(sess, saver_path)\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:39:24.030003Z",
     "start_time": "2020-03-31T15:39:14.878224Z"
    },
    "code_folding": [
     76,
     93
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLD-START BASELINE:\n",
      "\t Loss: 1.1857\n",
      "\t AUC: 0.6507\n",
      "INFO:tensorflow:Restoring parameters from saver/model-deepFM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 107.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline]\n",
      "\ttest-test loss:\t1.1113, improvement: 6.27%\n",
      "[baseline]\n",
      "\ttest-test auc:\t0.6780, improvement: 4.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 172.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline]\n",
      "\ttest-test loss:\t1.0522, improvement: 11.26%\n",
      "[baseline]\n",
      "\ttest-test auc:\t0.6934, improvement: 6.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 199.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline]\n",
      "\ttest-test loss:\t1.0070, improvement: 15.07%\n",
      "[baseline]\n",
      "\ttest-test auc:\t0.7022, improvement: 7.92%\n",
      "============================================================\n",
      "INFO:tensorflow:Restoring parameters from saver/model-deepFM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:02<00:00, 16.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss:\t1.0301, improvement: 13.12%\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc:\t0.6886, improvement: 5.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 184.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss:\t0.9780, improvement: 17.51%\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc:\t0.7005, improvement: 7.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 201.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta-Embedding]\n",
      "\ttest-test loss:\t0.9375, improvement: 20.93%\n",
      "[Meta-Embedding]\n",
      "\ttest-test auc:\t0.7074, improvement: 8.72%\n"
     ]
    }
   ],
   "source": [
    "print(\"COLD-START BASELINE:\")\n",
    "print(\"\\t Loss: {:.4f}\".format(logloss_base_cold))\n",
    "print(\"\\t AUC: {:.4f}\".format(auc_base_cold))\n",
    "'''\n",
    "Testing\n",
    "'''\n",
    "minibatchsize = MINIBATCHSIZE\n",
    "batch_n_ID = 25\n",
    "batchsize = minibatchsize * batch_n_ID\n",
    "i = 1\n",
    "test_n_ID = len(test_x_c[ID_col].drop_duplicates())\n",
    "saver.restore(sess, save_path)\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_a[i*batchsize:(i+1)*batchsize]\n",
    "    model.train_warm(sess, batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_base_batcha = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test loss:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_loss_test, 1-test_loss_test/logloss_base_cold))\n",
    "auc_base_batcha = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test auc:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_auc_test, test_auc_test/auc_base_cold-1))\n",
    "\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_b[i*batchsize:(i+1)*batchsize]\n",
    "    model.train_warm(sess, batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_base_batchb = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test loss:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_loss_test, 1-test_loss_test/logloss_base_cold))\n",
    "auc_base_batchb = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test auc:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_auc_test, test_auc_test/auc_base_cold-1))\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_c[i*batchsize:(i+1)*batchsize]\n",
    "    model.train_warm(sess, batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_base_batchc = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test loss:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_loss_test, 1-test_loss_test/logloss_base_cold))\n",
    "auc_base_batchc = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[baseline]\\n\\ttest-test auc:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_auc_test, test_auc_test/auc_base_cold-1))\n",
    "print(\"=\"*60)\n",
    "\n",
    "saver.restore(sess, save_path)\n",
    "\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_a[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_a[i*batchsize:(i+1)*batchsize]\n",
    "    aid = np.unique(batch_x[ID_col].values)\n",
    "    for k in range(batch_n_ID):\n",
    "        if k*minibatchsize>=len(batch_x):\n",
    "            break\n",
    "        ID = batch_x[ID_col].values[k*minibatchsize]\n",
    "        embeddings = model.get_meta_embedding(\n",
    "            sess, batch_x[k*minibatchsize:(k+1)*minibatchsize],\n",
    "            batch_t[k*minibatchsize:(k+1)*minibatchsize],\n",
    "            batch_g[k*minibatchsize:(k+1)*minibatchsize],\n",
    "        )\n",
    "        emb = embeddings.mean(0)\n",
    "        model.assign_meta_embedding(sess, ID, emb)\n",
    "    model.train_warm(sess, \n",
    "                     batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_ME_batcha = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_loss_test, 1-test_loss_test/logloss_base_cold))\n",
    "auc_ME_batcha = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_auc_test, test_auc_test/auc_base_cold-1))\n",
    "\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_b[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_b[i*batchsize:(i+1)*batchsize]\n",
    "    model.train_warm(sess, batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_ME_batchb = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_loss_test, 1-test_loss_test/logloss_base_cold))\n",
    "auc_ME_batchb = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_auc_test, test_auc_test/auc_base_cold-1))\n",
    "for i in tqdm(range(int(np.ceil(test_n_ID/batch_n_ID)))):\n",
    "    batch_x = test_x_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_t = test_t_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_g = test_g_c[i*batchsize:(i+1)*batchsize]\n",
    "    batch_y = test_y_c[i*batchsize:(i+1)*batchsize]\n",
    "    model.train_warm(sess, batch_x, batch_t, batch_g, batch_y, \n",
    "                     embedding_only=True)\n",
    "test_pred_test = predict_on_batch(sess, model.predict_warm, \n",
    "                                  test_x_test, test_t_test, test_g_test)\n",
    "logloss_ME_batchc = test_loss_test = log_loss(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test loss:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_loss_test, 1-test_loss_test/logloss_base_cold))\n",
    "auc_ME_batchc = test_auc_test = roc_auc_score(test_y_test, test_pred_test)\n",
    "print(\"[Meta-Embedding]\\n\\ttest-test auc:\\t{:.4f}, improvement: {:.2%}\".format(\n",
    "    test_auc_test, test_auc_test/auc_base_cold-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:39:24.037736Z",
     "start_time": "2020-03-31T15:39:24.032364Z"
    }
   },
   "outputs": [],
   "source": [
    "# write the scores into file.\n",
    "res = [logloss_base_cold, logloss_ME_cold, \n",
    "       logloss_base_batcha, logloss_ME_batcha, \n",
    "       logloss_base_batchb, logloss_ME_batchb, \n",
    "       logloss_base_batchc, logloss_ME_batchc, \n",
    "       auc_base_cold, auc_ME_cold, \n",
    "       auc_base_batcha, auc_ME_batcha, \n",
    "       auc_base_batchb, auc_ME_batchb, \n",
    "       auc_base_batchc, auc_ME_batchc]\n",
    "with open(LOG, \"a\") as logfile:\n",
    "    logfile.writelines(\",\".join([str(x) for x in res])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
